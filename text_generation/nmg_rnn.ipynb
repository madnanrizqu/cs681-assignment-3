{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srXC6pLGLwS6"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "yG_n40gFzf9s"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CONFIG = {\n",
        "    # Data processing\n",
        "    \"SEED\": 42,                     # Random seed for reproducibility\n",
        "    \"TRAIN_SPLIT\": 0.8,             # Percentage of data for training\n",
        "    \"VAL_SPLIT\": 0.1,              # Percentage of data for validation\n",
        "    \"TEST_SPLIT\": 0.1,              # Percentage of data for testing\n",
        "    \"SEQ_LENGTH\": 100,              # Sequence length for training examples\n",
        "    \n",
        "    # Model architecture\n",
        "    \"EMBEDDING_DIM\": 256,           # Dimension of the embedding layer\n",
        "    \"RNN_UNITS\": 1024,              # Number of units in the RNN layer\n",
        "    \n",
        "    # Training parameters\n",
        "    \"BATCH_SIZE\": 64,               # Batch size for training\n",
        "    \"BUFFER_SIZE\": 10000,           # Buffer size for shuffling\n",
        "    \"EPOCHS\": 20,                   # Number of epochs for training\n",
        "    \"OPTIMIZER\": \"adam\",            # Optimizer for training\n",
        "    \"EARLY_STOPPING_PATIENCE\": 5,   # Patience for early stopping\n",
        "    \"MONITOR_METRIC\": \"val_loss\",   # Metric to monitor for early stopping\n",
        "    \"RESTORE_BEST_WEIGHTS\": True,   # Whether to restore best weights after training\n",
        "    \n",
        "    # Text generation\n",
        "    \"TEMPERATURE\": 1.0,             # Temperature for text generation\n",
        "    \"GENERATION_LENGTH\": 1000       # Length of generated text\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seeds set to 42 for reproducibility\n"
          ]
        }
      ],
      "source": [
        "# Set seeds for reproducibility\n",
        "def set_seeds(seed=42):\n",
        "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
        "    # Set seed for Python's random module\n",
        "    random.seed(seed)\n",
        "    \n",
        "    # Set seed for NumPy\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    # Set seed for TensorFlow\n",
        "    tf.random.set_seed(seed)\n",
        "    \n",
        "    # Try to make operations deterministic (TF 2.8+)\n",
        "    try:\n",
        "        tf.config.experimental.enable_op_determinism()\n",
        "    except:\n",
        "        # For older TensorFlow versions\n",
        "        print(\"Warning: Op determinism not available in your TF version. Results may still vary.\")\n",
        "        # Set as many deterministic settings as possible\n",
        "        os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "        \n",
        "    print(f\"Seeds set to {seed} for reproducibility\")\n",
        "\n",
        "set_seeds(CONFIG[\"SEED\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHDoRoc5PKWz"
      },
      "source": [
        "### Download the Shakespeare dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "pD_55cOxLkAb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading Romeo and Juliet text...\n",
            "Download completed successfully.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "# URL of the dataset\n",
        "url = \"https://www.gutenberg.org/cache/epub/1513/pg1513.txt\"\n",
        "\n",
        "# Download the file\n",
        "print(\"Downloading Romeo and Juliet text...\")\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "  if not os.path.exists('./dataset'):\n",
        "    os.makedirs('./dataset')\n",
        "\n",
        "  path_to_file = \"./dataset/romeo_and_juliet.txt\"\n",
        "  with open(path_to_file, 'wb') as f:\n",
        "    f.write(response.content)\n",
        "  print(\"Download completed successfully.\")\n",
        "else:\n",
        "  raise Exception(f\"Failed to download file. Status code: {response.status_code}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHjdCjDuSvX_"
      },
      "source": [
        "### Read the data\n",
        "\n",
        "First, look in the text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "aavnuByVymwK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of text: 167424 characters\n"
          ]
        }
      ],
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text length: 167424 characters\n",
            "Processed text length: 147650 characters\n",
            "Removed 19774 characters of metadata\n"
          ]
        }
      ],
      "source": [
        "def preprocess_romeo_and_juliet(text):\n",
        "    \"\"\"\n",
        "    Extract only the actual play content from the Romeo and Juliet text,\n",
        "    removing Project Gutenberg header, footer, and metadata.\n",
        "    \"\"\"\n",
        "    # Find the beginning of the actual play\n",
        "    start_marker = \"THE PROLOGUE\"\n",
        "    end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
        "    \n",
        "    start_index = text.find(start_marker)\n",
        "    end_index = text.find(end_marker)\n",
        "    \n",
        "    if start_index == -1 or end_index == -1:\n",
        "        print(\"Warning: Could not find start or end markers in the text\")\n",
        "        return text\n",
        "    \n",
        "    # Extract just the play content\n",
        "    play_text = text[start_index:end_index].strip()\n",
        "    \n",
        "    print(f\"Original text length: {len(text)} characters\")\n",
        "    print(f\"Processed text length: {len(play_text)} characters\")\n",
        "    print(f\"Removed {len(text) - len(play_text)} characters of metadata\")\n",
        "    \n",
        "    return play_text\n",
        "\n",
        "# Apply preprocessing to remove header and footer\n",
        "text = preprocess_romeo_and_juliet(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "Duhg9NrUymwO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 250 characters of the processed text:\n",
            "THE PROLOGUE.\n",
            "\n",
            "ACT I\n",
            "Scene I. A public place.\n",
            "Scene II. A Street.\n",
            "Scene III. Room in Capulet’s House.\n",
            "Scene IV. A Street.\n",
            "Scene V. A Hall in Capulet’s House.\n",
            "\n",
            "ACT II\n",
            "CHORUS.\n",
            "Scene I. An open place adjoining Capulet’s Garden.\n",
            "Scene II. Cap\n",
            "\n",
            "Last 250 characters of the processed text:\n",
            "s morning with it brings;\n",
            "The sun for sorrow will not show his head.\n",
            "Go hence, to have more talk of these sad things.\n",
            "Some shall be pardon’d, and some punished,\n",
            "For never was a story of more woe\n",
            "Than this of Juliet and her Romeo.\n",
            "\n",
            " [_Exeunt._]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Take a look at the beginning and end of the processed text\n",
        "print(\"First 250 characters of the processed text:\")\n",
        "print(text[:250])\n",
        "\n",
        "print(\"\\nLast 250 characters of the processed text:\")\n",
        "print(text[-250:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "IlCgQBRVymwR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "71 unique characters\n"
          ]
        }
      ],
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNnrKn_lL-IJ"
      },
      "source": [
        "## Process the text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFjSVAlWzf-N"
      },
      "source": [
        "### Vectorize the text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This converts from tokens to character IDs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "6GMlCe3qzaL9"
      },
      "outputs": [],
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This recovers the characters from the vectors of IDs, and returns them as a `tf.RaggedTensor` of characters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "Wd2m3mqkDjRj"
      },
      "outputs": [],
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FeW5gqutT3o"
      },
      "source": [
        "Can use `tf.strings.reduce_join` to join the characters back into strings. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "w5apvBDn9Ind"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgsVvVxnymwf"
      },
      "source": [
        "### Create training examples and targets\n",
        "\n",
        "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right. So break the text into chunks of `seq_length+1`\n",
        "\n",
        "Example with \"Tensorflow\":\n",
        "The string \"Tensorflow\" is split into:\n",
        "\n",
        "Input: All characters except the last one.\n",
        "Target: All characters except the first one.\n",
        "Step-by-Step Breakdown:\n",
        "Original Sequence: \"Tensorflow\"\n",
        "\n",
        "Characters: ['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w']\n",
        "Input Sequence:\n",
        "\n",
        "Take all characters except the last one: ['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o']\n",
        "Target Sequence:\n",
        "\n",
        "Take all characters except the first one: ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w']\n",
        "\n",
        "Each character in the Input corresponds to the Target character at the next position:\n",
        "\n",
        "```\n",
        "'T' → 'e'\n",
        "'e' → 'n'\n",
        "'n' → 's'\n",
        "'s' → 'o'\n",
        "'o' → 'r'\n",
        "'r' → 'f'\n",
        "'f' → 'l'\n",
        "'l' → 'o'\n",
        "'o' → 'w'\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "UopbsKi88tm5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(147650,), dtype=int64, numpy=array([31, 19, 16, ...,  8, 39, 38])>"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "qmxrYDCTy-eL"
      },
      "outputs": [],
      "source": [
        "# Create a 70/15/15 train/validation/test split\n",
        "total_chars = len(all_ids)\n",
        "train_chars = int(CONFIG[\"TRAIN_SPLIT\"] * total_chars)\n",
        "val_chars = int(CONFIG[\"VAL_SPLIT\"] * total_chars)\n",
        "\n",
        "# Split the data\n",
        "train_ids = all_ids[:train_chars]\n",
        "val_ids = all_ids[train_chars:train_chars+val_chars]\n",
        "test_ids = all_ids[train_chars+val_chars:]\n",
        "\n",
        "# Create separate datasets\n",
        "train_ids_dataset = tf.data.Dataset.from_tensor_slices(train_ids)\n",
        "val_ids_dataset = tf.data.Dataset.from_tensor_slices(val_ids)\n",
        "test_ids_dataset = tf.data.Dataset.from_tensor_slices(test_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZSYAcQV8OGP"
      },
      "source": [
        "The `batch` method lets you easily convert these individual characters to sequences of the desired size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "BpdjRO2CzOfZ"
      },
      "outputs": [],
      "source": [
        "# Create sequences for each dataset\n",
        "train_sequences = train_ids_dataset.batch(CONFIG[\"SEQ_LENGTH\"]+1, drop_remainder=True)\n",
        "val_sequences = val_ids_dataset.batch(CONFIG[\"SEQ_LENGTH\"]+1, drop_remainder=True)\n",
        "test_sequences = test_ids_dataset.batch(CONFIG[\"SEQ_LENGTH\"]+1, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PHW902-4oZt"
      },
      "source": [
        "It's easier to see what this is doing if you join the tokens back into strings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "QO32cMWu4a06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'THE PROLOGUE.\\r\\n\\r\\nACT I\\r\\nScene I. A public place.\\r\\nScene II. A Street.\\r\\nScene III. Room in Capulet\\xe2\\x80\\x99s H'\n",
            "b'ouse.\\r\\nScene IV. A Street.\\r\\nScene V. A Hall in Capulet\\xe2\\x80\\x99s House.\\r\\n\\r\\nACT II\\r\\nCHORUS.\\r\\nScene I. An open '\n",
            "b'place adjoining Capulet\\xe2\\x80\\x99s Garden.\\r\\nScene II. Capulet\\xe2\\x80\\x99s Garden.\\r\\nScene III. Friar Lawrence\\xe2\\x80\\x99s Cell.\\r\\nSc'\n",
            "b'ene IV. A Street.\\r\\nScene V. Capulet\\xe2\\x80\\x99s Garden.\\r\\nScene VI. Friar Lawrence\\xe2\\x80\\x99s Cell.\\r\\n\\r\\nACT III\\r\\nScene I. '\n",
            "b'A public Place.\\r\\nScene II. A Room in Capulet\\xe2\\x80\\x99s House.\\r\\nScene III. Friar Lawrence\\xe2\\x80\\x99s cell.\\r\\nScene IV. A'\n"
          ]
        }
      ],
      "source": [
        "for seq in train_sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbLcIPBj_mWZ"
      },
      "source": [
        "For training you'll need a dataset of `(input, label)` pairs. Where `input` and \n",
        "`label` are sequences. At each time step the input is the current character and the label is the next character. \n",
        "\n",
        "Here's a function that takes a sequence as input, duplicates, and shifts it to align the input and label for each timestep:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "9NGu-FkO_kYU"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "B9iKPXkw5xwa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input : b'THE PROLOGUE.\\r\\n\\r\\nACT I\\r\\nScene I. A public place.\\r\\nScene II. A Street.\\r\\nScene III. Room in Capulet\\xe2\\x80\\x99s '\n",
            "Target: b'HE PROLOGUE.\\r\\n\\r\\nACT I\\r\\nScene I. A public place.\\r\\nScene II. A Street.\\r\\nScene III. Room in Capulet\\xe2\\x80\\x99s H'\n"
          ]
        }
      ],
      "source": [
        "# Convert sequences to input-target pairs\n",
        "train_dataset = train_sequences.map(split_input_target)\n",
        "val_dataset = val_sequences.map(split_input_target)\n",
        "test_dataset = test_sequences.map(split_input_target)\n",
        "\n",
        "# Check a sample\n",
        "for input_example, target_example in train_dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJdfPmdqzf-R"
      },
      "source": [
        "### Create training batches\n",
        "\n",
        "You used `tf.data` to split the text into manageable sequences. But before feeding this data into the model, you need to shuffle the data and pack it into batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "p2pGotuNzf-S"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training dataset: <_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>\n",
            "Validation dataset: <_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>\n",
            "Test dataset: <_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>\n"
          ]
        }
      ],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "# Apply batching to all datasets\n",
        "train_dataset = (\n",
        "    train_dataset\n",
        "    .shuffle(CONFIG[\"BUFFER_SIZE\"])\n",
        "    .batch(CONFIG[\"BATCH_SIZE\"], drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "# For validation and test, we don't need to shuffle\n",
        "val_dataset = (\n",
        "    val_dataset\n",
        "    .batch(CONFIG[\"BATCH_SIZE\"], drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "test_dataset = (\n",
        "    test_dataset\n",
        "    .batch(CONFIG[\"BATCH_SIZE\"], drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "print(\"Training dataset:\", train_dataset)\n",
        "print(\"Validation dataset:\", val_dataset)\n",
        "print(\"Test dataset:\", test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "## Build The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "zHT8cLh7EAsg"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "wj8HQ2w8z4iO"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__()\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.rnn = tf.keras.layers.SimpleRNN(rnn_units,\n",
        "                                         return_sequences=True,\n",
        "                                         return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    \n",
        "    # Get batch size from input tensor\n",
        "    batch_size = tf.shape(x)[0]\n",
        "    \n",
        "    if states is None:\n",
        "      # Manually create initial state with correct shape [batch_size, rnn_units]\n",
        "      states = tf.zeros([batch_size, self.rnn.units])\n",
        "    \n",
        "    x, states = self.rnn(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "IX58Xj9z47Aw"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=CONFIG[\"EMBEDDING_DIM\"],\n",
        "    rnn_units=CONFIG[\"RNN_UNITS\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "vPGmAAXmVLGC"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"my_model_2\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"my_model_2\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,432</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ simple_rnn_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        │ ((<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>,   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,311,744</span> │\n",
              "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>))                 │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>)           │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,800</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)          │        \u001b[38;5;34m18,432\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ simple_rnn_2 (\u001b[38;5;33mSimpleRNN\u001b[0m)        │ ((\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1024\u001b[0m), (\u001b[38;5;34m1\u001b[0m,   │     \u001b[38;5;34m1,311,744\u001b[0m │\n",
              "│                                 │ \u001b[38;5;34m1024\u001b[0m))                 │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m72\u001b[0m)           │        \u001b[38;5;34m73,800\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,403,976</span> (5.36 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,403,976\u001b[0m (5.36 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,403,976</span> (5.36 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,403,976\u001b[0m (5.36 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create a dummy input with the right shape\n",
        "# So that the custom class model can by analyzed by .summary()\n",
        "dummy_input = tf.zeros((1, CONFIG[\"SEQ_LENGTH\"]), dtype=tf.int64)\n",
        "model(dummy_input) \n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "ZOeWdgxNFDXq"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "DDl1_Een6rL0"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=CONFIG[\"OPTIMIZER\"], loss=loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ky3F_BhgkTW"
      },
      "source": [
        "### Execute the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a function to calculate perplexity\n",
        "def calculate_perplexity(model, dataset):\n",
        "    \"\"\"\n",
        "    Calculate perplexity on a dataset.\n",
        "    Perplexity = exp(average cross-entropy loss)\n",
        "    \"\"\"\n",
        "    total_loss = 0\n",
        "    total_samples = 0\n",
        "    \n",
        "    for input_batch, target_batch in dataset:\n",
        "        predictions = model(input_batch)\n",
        "        # Get batch_size and sequence_length\n",
        "        batch_size, sequence_length = target_batch.shape\n",
        "        \n",
        "        # Calculate loss for each prediction\n",
        "        batch_loss = loss(target_batch, predictions)\n",
        "        total_loss += batch_loss * batch_size\n",
        "        total_samples += batch_size\n",
        "        \n",
        "    # Calculate average loss\n",
        "    avg_loss = total_loss / total_samples\n",
        "    \n",
        "    # Perplexity is exp(average loss)\n",
        "    perplexity = tf.exp(avg_loss)\n",
        "    \n",
        "    return perplexity.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PerplexityCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, val_dataset):\n",
        "        super().__init__()\n",
        "        self.val_dataset = val_dataset\n",
        "        self.perplexity_history = []\n",
        "        \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        perplexity = calculate_perplexity(self.model, self.val_dataset)\n",
        "        self.perplexity_history.append(perplexity)\n",
        "        logs['val_perplexity'] = perplexity\n",
        "        print(f\"\\nValidation Perplexity: {perplexity:.4f}\")\n",
        "\n",
        "# Create the callback\n",
        "perplexity_callback = PerplexityCallback(val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=CONFIG[\"MONITOR_METRIC\"],\n",
        "    patience=CONFIG[\"EARLY_STOPPING_PATIENCE\"],\n",
        "    restore_best_weights=CONFIG[\"RESTORE_BEST_WEIGHTS\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "UK-hmKjYVoll"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326ms/step - loss: 3.9527\n",
            "Validation Perplexity: 27.3416\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 379ms/step - loss: 3.9381 - val_loss: 3.3084 - val_perplexity: 27.3416\n",
            "Epoch 2/10\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375ms/step - loss: 3.2397\n",
            "Validation Perplexity: 16.5003\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 418ms/step - loss: 3.2322 - val_loss: 2.8034 - val_perplexity: 16.5003\n",
            "Epoch 3/10\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375ms/step - loss: 2.6384\n",
            "Validation Perplexity: 12.2303\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 416ms/step - loss: 2.6344 - val_loss: 2.5039 - val_perplexity: 12.2303\n",
            "Epoch 4/10\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 313ms/step - loss: 2.3858\n",
            "Validation Perplexity: 10.5487\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 354ms/step - loss: 2.3833 - val_loss: 2.3560 - val_perplexity: 10.5487\n",
            "Epoch 5/10\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 357ms/step - loss: 2.2235\n",
            "Validation Perplexity: 9.5891\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 402ms/step - loss: 2.2231 - val_loss: 2.2606 - val_perplexity: 9.5891\n",
            "Epoch 6/10\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 350ms/step - loss: 2.1312\n",
            "Validation Perplexity: 8.8682\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 391ms/step - loss: 2.1303 - val_loss: 2.1825 - val_perplexity: 8.8682\n",
            "Epoch 7/10\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374ms/step - loss: 2.0517\n",
            "Validation Perplexity: 8.4702\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 426ms/step - loss: 2.0511 - val_loss: 2.1366 - val_perplexity: 8.4702\n",
            "Epoch 8/10\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383ms/step - loss: 1.9990\n",
            "Validation Perplexity: 7.9649\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 433ms/step - loss: 1.9986 - val_loss: 2.0750 - val_perplexity: 7.9649\n",
            "Epoch 9/10\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 344ms/step - loss: 1.9306\n",
            "Validation Perplexity: 7.6021\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 381ms/step - loss: 1.9302 - val_loss: 2.0284 - val_perplexity: 7.6021\n",
            "Epoch 10/10\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348ms/step - loss: 1.8781\n",
            "Validation Perplexity: 7.3648\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 389ms/step - loss: 1.8779 - val_loss: 1.9967 - val_perplexity: 7.3648\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    train_dataset, \n",
        "    epochs=CONFIG[\"EPOCHS\"], \n",
        "    validation_data=val_dataset,\n",
        "    callbacks=[early_stopping, perplexity_callback],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on test set...\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - loss: 1.9402\n",
            "Test loss: 1.9394915103912354\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on test set\n",
        "print(\"Evaluating on test set...\")\n",
        "test_loss = model.evaluate(test_dataset)\n",
        "print(f\"Test loss: {test_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Perplexity: 6.9552\n"
          ]
        }
      ],
      "source": [
        "# Calculate perplexity on test set\n",
        "test_perplexity = calculate_perplexity(model, test_dataset)\n",
        "print(f\"Test Perplexity: {test_perplexity:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "## Generate text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "iSBU1tHmlUSs"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=None):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature if temperature is not None else CONFIG[\"TEMPERATURE\"]\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                        return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "fqMOuDutnOxK"
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "ST7PSyk9t1mT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROMEO.\n",
            "What. Doun, Mondagks.\n",
            "Butter, toul, ther leveron, theich this ofay.\n",
            "\n",
            "BENVO.\n",
            "Labing\n",
            "Jxo wherf art breach seal] now harlive hears, onjerer tove as wother’’g hove peat for hor live ary shaiseranes yous wiod.\n",
            "I’w the fover. Whict the colint? shay had, ay’ myel.\n",
            "\n",
            "JULIET.\n",
            "[_y, whin shoughe\n",
            "’ftuties nove you.\n",
            "\n",
            "BULIET.\n",
            "\n",
            "MENTOMEO.\n",
            "Goof it hingo oursy, eny hil?\n",
            "\n",
            "LAPUOET.\n",
            "I pherp that wabr,\n",
            "Shat wallisil. Whom sorewel of my I’frs.\n",
            "\n",
            "toul perto ghamurse you but fartlems, a redousd do tofut on vore ox of heave-krend fithtuns that a sither\n",
            "Withe so grow thou.\n",
            "\n",
            "ScENCE.\n",
            "Me rave all dog.\n",
            "Folf in oy; Muspridsans as shear what wath, lovest swiel: peavyer nove on?\n",
            "The whou bast?\n",
            "Prows toll piod a kay were at if thy purse.\n",
            "\n",
            " noulb.\n",
            "\n",
            " Entul that se—t is s’sidstw and timt you hould-and be.\n",
            "Gorsat, wour youll nod hive and ow thoush sher, goom,\n",
            "Thy faint?\n",
            "I parser dest ges hand, I buppinigiteing-walliv, I willing ond, not shemes hichion.\n",
            "\n",
            "F[RASCE.\n",
            "\n",
            "P[_AR\n",
            "Sh shar ase gsin \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 0.7134771347045898\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO.'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(CONFIG[\"GENERATION_LENGTH\"]):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "text_generation.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "cs681-3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
