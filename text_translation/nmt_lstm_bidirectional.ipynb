{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "tnxXKDjq3jEL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import einops\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n",
        "\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "CONFIG = {\n",
        "    # Reproducibility\n",
        "    \"SEED\": 42,  # Controls randomization for reproducible results\n",
        "    \n",
        "    # Data parameters\n",
        "    \"BATCH_SIZE\": 64,  # Batch size for training and evaluation\n",
        "    \"MAX_VOCAB_SIZE\": 5000,  # Maximum vocabulary size for text processors\n",
        "    \"TRAIN_SPLIT\": 0.8,  # 80% training, 20% validation split\n",
        "    \n",
        "    # Model architecture\n",
        "    \"UNITS\": 256,  # Dimension of embeddings and hidden LSTM units\n",
        "    \"ATTENTION_HEADS\": 4,  # Number of attention heads in cross-attention\n",
        "    \n",
        "    # Training parameters\n",
        "    \"OPTIMIZER\": \"adam\",  # Optimizer algorithm\n",
        "    \"EPOCHS\": 20,  # Number of training epochs\n",
        "    \"STEPS_PER_EPOCH\": 100,  # Number of steps per epoch\n",
        "    \"VALIDATION_STEPS\": 20,  # Number of validation steps per epoch\n",
        "    \"EARLY_STOPPING_PATIENCE\": 3,  # Number of epochs with no improvement before stopping\n",
        "    \n",
        "    # Translation/Testing parameters\n",
        "    \"MAX_LENGTH\": 50,  # Maximum length of generated translations\n",
        "    \"TEMPERATURE\": 0.0,  # Controls randomness in generation (0.0 = deterministic)\n",
        "    \"TEST_SAMPLE_COUNT\": 5,  # Number of examples to display during testing\n",
        "    \"BLEU_SAMPLE_SIZE\": 100  # Number of samples to evaluate for BLEU score\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random seeds set to 42 for reproducible results\n"
          ]
        }
      ],
      "source": [
        "# Set seeds for all random number generators\n",
        "random.seed(CONFIG[\"SEED\"])\n",
        "np.random.seed(CONFIG[\"SEED\"])\n",
        "tf.random.set_seed(CONFIG[\"SEED\"])\n",
        "\n",
        "print(f\"Random seeds set to {CONFIG['SEED']} for reproducible results\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRVATYOgJs1b"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "\n",
        "# URL for the dataset\n",
        "url = \"https://www.manythings.org/anki/ind-eng.zip\"\n",
        "\n",
        "# Define a directory to store the downloaded files\n",
        "data_dir = pathlib.Path(os.path.join(os.getcwd(), 'dataset'))\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "data_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Path for the extracted text file\n",
        "path_to_file = data_dir / 'ind.txt'\n",
        "\n",
        "# Download and extract only if the file doesn't exist\n",
        "if not path_to_file.exists():\n",
        "    print(f\"Downloading dataset from {url}...\")\n",
        "    \n",
        "    # Download the zip file with headers that mimic a browser\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "        'Accept-Language': 'en-US,en;q=0.5'\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "        \n",
        "    # Check if the download was successful\n",
        "    if response.status_code == 200:\n",
        "        # Extract the zip file\n",
        "        print(\"Extracting files...\")\n",
        "        with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:\n",
        "            # Extract only the ind.txt file\n",
        "            for file in zip_ref.namelist():\n",
        "                if file.endswith('ind.txt'):\n",
        "                    zip_ref.extract(file, data_dir)\n",
        "                    # Move the file if it's in a subdirectory\n",
        "                    extracted_path = data_dir / file\n",
        "                    if str(extracted_path) != str(path_to_file):\n",
        "                        os.rename(extracted_path, path_to_file)\n",
        "                    break\n",
        "        print(\"Dataset extracted successfully.\")\n",
        "    else:\n",
        "        raise Exception(f\"Failed to download the dataset. Status code: {response.status_code}\")\n",
        "else:\n",
        "    print(f\"Dataset file already exists at {path_to_file}\")\n",
        "\n",
        "# Verify the file exists\n",
        "if not path_to_file.exists():\n",
        "  raise FileNotFoundError(f\"Dataset file not found at {path_to_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "OHn4Dct23jEm"
      },
      "outputs": [],
      "source": [
        "def load_data(path):\n",
        "  text = path.read_text(encoding='utf-8')\n",
        "\n",
        "  lines = text.splitlines()\n",
        "  # The format is: English<tab>Indonesian<tab>Attribution info\n",
        "  # We only need the first two columns\n",
        "  pairs = [line.split('\\t')[:2] for line in lines]\n",
        "\n",
        "  # English is the source language (context), Indonesian is the target\n",
        "  context = np.array([source for source, target in pairs])\n",
        "  target = np.array([target for source, target in pairs])\n",
        "\n",
        "  return target, context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "cTbSbBz55QtF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "If a person has not had a chance to acquire his target language by the time he's an adult, he's unlikely to be able to reach native speaker level in that language.\n"
          ]
        }
      ],
      "source": [
        "target_raw, context_raw = load_data(path_to_file)\n",
        "print(context_raw[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "lH_dPY8TRp3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jika seseorang tidak berkesempatan untuk menguasai bahasa yang diinginkannya ketika menginjak dewasa, maka kecil kemungkinan ia akan bisa mencapai tingkatan penutur asli dalam bahasa tersebut.\n"
          ]
        }
      ],
      "source": [
        "print(target_raw[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "3rZFgz69nMPa"
      },
      "outputs": [],
      "source": [
        "# When creating train and validation datasets\n",
        "is_train = np.random.uniform(size=(len(target_raw),)) < CONFIG[\"TRAIN_SPLIT\"]\n",
        "\n",
        "# Set buffer size dynamically and store in CONFIG\n",
        "CONFIG[\"BUFFER_SIZE\"] = len(context_raw)\n",
        "\n",
        "train_raw = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((context_raw[is_train], target_raw[is_train]))\n",
        "    .shuffle(CONFIG[\"BUFFER_SIZE\"])\n",
        "    .batch(CONFIG[\"BATCH_SIZE\"]))\n",
        "val_raw = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((context_raw[~is_train], target_raw[~is_train]))\n",
        "    .shuffle(CONFIG[\"BUFFER_SIZE\"])\n",
        "    .batch(CONFIG[\"BATCH_SIZE\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "qc6-NK1GtWQt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b\"She's unconscious.\" b'I ran into an old friend.'\n",
            " b'In the near future, we will be able to put an end to AIDS.'\n",
            " b'Tom recommended that I apply for the job.'\n",
            " b'When did you learn to make pizza?'], shape=(5,), dtype=string)\n",
            "\n",
            "tf.Tensor(\n",
            "[b'Dia tidak sadar.' b'Aku berjumpa dengan kawan lamaku.'\n",
            " b'Dalam waktu dekat ini kita akan mampu mengobati AIDS'\n",
            " b'Tom merekomendasikan saya untuk melamar pekerjaan.'\n",
            " b'Kapan kamu belajar membuat pizza'], shape=(5,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "for example_context_strings, example_target_strings in train_raw.take(1):\n",
        "  print(example_context_strings[:5])\n",
        "  print()\n",
        "  print(example_target_strings[:5])\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "chTF5N885F0P"
      },
      "outputs": [],
      "source": [
        "def tf_lower_and_split_punct(text):\n",
        "  # Split accented characters.\n",
        "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  text = tf.strings.lower(text)\n",
        "  # Keep space, a to z, and select punctuation.\n",
        "  # This regex should work fine for Indonesian\n",
        "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "  # Add spaces around punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
        "  # Strip whitespace.\n",
        "  text = tf.strings.strip(text)\n",
        "\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "eAY9k49G3jE_"
      },
      "outputs": [],
      "source": [
        "context_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=CONFIG[\"MAX_VOCAB_SIZE\"],\n",
        "    ragged=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "bmsI1Yql8FYe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " np.str_('[START]'),\n",
              " np.str_('[END]'),\n",
              " np.str_('.'),\n",
              " np.str_('i'),\n",
              " np.str_('tom'),\n",
              " np.str_('the'),\n",
              " np.str_('you'),\n",
              " np.str_('to')]"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "context_text_processor.adapt(train_raw.map(lambda context, target: context))\n",
        "\n",
        "# Here are the first 10 words from the vocabulary:\n",
        "context_text_processor.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "jlC4xuZnKLBS"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-22 20:44:05.350803: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " np.str_('[START]'),\n",
              " np.str_('[END]'),\n",
              " np.str_('.'),\n",
              " np.str_('tom'),\n",
              " np.str_('aku'),\n",
              " np.str_('?'),\n",
              " np.str_('tidak'),\n",
              " np.str_('yang')]"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=CONFIG[\"MAX_VOCAB_SIZE\"],\n",
        "    ragged=True)\n",
        "\n",
        "target_text_processor.adapt(train_raw.map(lambda context, target: target))\n",
        "target_text_processor.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "9KZxj8IrNZ9S"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[2, 236, 1510, 4, 3], [2, 5, 264, 209, 79, 159, 244, 4, 3],\n",
              " [2, 14, 7, 582, 1077, 16, 32, 60, 34, 338, 9, 237, 79, 807, 9, 4478, 4, 3]]>"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_tokens = context_text_processor(example_context_strings)\n",
        "example_tokens[:3, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "wk5tbZWQl5u1"
      },
      "outputs": [],
      "source": [
        "def process_text(context, target):\n",
        "  context = context_text_processor(context).to_tensor()\n",
        "  target = target_text_processor(target)\n",
        "  targ_in = target[:,:-1].to_tensor()\n",
        "  targ_out = target[:,1:].to_tensor()\n",
        "  return (context, targ_in), targ_out\n",
        "\n",
        "\n",
        "train_ds = train_raw.map(process_text, tf.data.AUTOTUNE)\n",
        "val_ds = val_raw.map(process_text, tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ShapeChecker():\n",
        "  def __init__(self):\n",
        "    # Keep a cache of every axis-name seen\n",
        "    self.shapes = {}\n",
        "\n",
        "  def __call__(self, tensor, names, broadcast=False):\n",
        "    if not tf.executing_eagerly():\n",
        "      return\n",
        "\n",
        "    parsed = einops.parse_shape(tensor, names)\n",
        "\n",
        "    for name, new_dim in parsed.items():\n",
        "      old_dim = self.shapes.get(name, None)\n",
        "      \n",
        "      if (broadcast and new_dim == 1):\n",
        "        continue\n",
        "\n",
        "      if old_dim is None:\n",
        "        # If the axis name is new, add its length to the cache.\n",
        "        self.shapes[name] = new_dim\n",
        "        continue\n",
        "\n",
        "      if new_dim != old_dim:\n",
        "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
        "                         f\"    found: {new_dim}\\n\"\n",
        "                         f\"    expected: {old_dim}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "nZ2rI24i3jFg"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, text_processor, units):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.text_processor = text_processor\n",
        "    self.vocab_size = text_processor.vocabulary_size()\n",
        "    self.units = units\n",
        "    \n",
        "    # The embedding layer converts tokens to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.vocab_size, units,\n",
        "                                               mask_zero=True)\n",
        "\n",
        "    self.rnn = tf.keras.layers.Bidirectional(\n",
        "        merge_mode='sum',\n",
        "        layer=tf.keras.layers.LSTM(units,\n",
        "                            # Return the sequence and state\n",
        "                            return_sequences=True,\n",
        "                            recurrent_initializer='glorot_uniform'))\n",
        "\n",
        "  def call(self, x):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(x, 'batch s')\n",
        "\n",
        "    # 2. The embedding layer looks up the embedding vector for each token.\n",
        "    x = self.embedding(x)\n",
        "    shape_checker(x, 'batch s units')\n",
        "\n",
        "    # 3. The LSTM processes the sequence of embeddings.\n",
        "    x = self.rnn(x)\n",
        "    shape_checker(x, 'batch s units')\n",
        "\n",
        "    # 4. Returns the new sequence of embeddings.\n",
        "    return x\n",
        "\n",
        "  def convert_input(self, texts):\n",
        "    texts = tf.convert_to_tensor(texts)\n",
        "    if len(texts.shape) == 0:\n",
        "      texts = tf.convert_to_tensor(texts)[tf.newaxis]\n",
        "    context = self.text_processor(texts).to_tensor()\n",
        "    context = self(context)\n",
        "    return context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[   2    5 1017  102 3452  210    4    3    0    0]\n",
            "\n",
            "[   2    6  630   45  198 1218    4    0    0    0]\n",
            "[   6  630   45  198 1218    4    3    0    0    0]\n"
          ]
        }
      ],
      "source": [
        "for (ex_context_tok, ex_tar_in), ex_tar_out in train_ds.take(1):\n",
        "  print(ex_context_tok[0, :10].numpy()) \n",
        "  print()\n",
        "  print(ex_tar_in[0, :10].numpy()) \n",
        "  print(ex_tar_out[0, :10].numpy()) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "60gSVh05Jl6l"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context tokens, shape (batch, s): (64, 19)\n",
            "Encoder output, shape (batch, s, units): (64, 19, 256)\n"
          ]
        }
      ],
      "source": [
        "# Encode the input sequence.\n",
        "encoder = Encoder(context_text_processor, CONFIG[\"UNITS\"])\n",
        "ex_context = encoder(ex_context_tok)\n",
        "\n",
        "print(f'Context tokens, shape (batch, s): {ex_context_tok.shape}')\n",
        "print(f'Encoder output, shape (batch, s, units): {ex_context.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "-Ql3ymqwD8LS"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(\n",
        "        key_dim=units, \n",
        "        num_heads=CONFIG[\"ATTENTION_HEADS\"], \n",
        "        **kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "  def call(self, x, context):\n",
        "    shape_checker = ShapeChecker()\n",
        " \n",
        "    shape_checker(x, 'batch t units')\n",
        "    shape_checker(context, 'batch s units')\n",
        "\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,\n",
        "        value=context,\n",
        "        return_attention_scores=True)\n",
        "    \n",
        "    shape_checker(x, 'batch t units')\n",
        "    shape_checker(attn_scores, 'batch heads t s')\n",
        "    \n",
        "    # Cache the attention scores for plotting later.\n",
        "    attn_scores = tf.reduce_mean(attn_scores, axis=1)\n",
        "    shape_checker(attn_scores, 'batch t s')\n",
        "    self.last_attention_weights = attn_scores\n",
        "\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "erYvHIgAl8kh"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, text_processor, units):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.text_processor = text_processor\n",
        "    self.vocab_size = text_processor.vocabulary_size()\n",
        "    self.word_to_id = tf.keras.layers.StringLookup(\n",
        "        vocabulary=text_processor.get_vocabulary(),\n",
        "        mask_token='', oov_token='[UNK]')\n",
        "    self.id_to_word = tf.keras.layers.StringLookup(\n",
        "        vocabulary=text_processor.get_vocabulary(),\n",
        "        mask_token='', oov_token='[UNK]',\n",
        "        invert=True)\n",
        "    self.start_token = self.word_to_id('[START]')\n",
        "    self.end_token = self.word_to_id('[END]')\n",
        "\n",
        "    self.units = units\n",
        "\n",
        "    # 1. The embedding layer converts token IDs to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.vocab_size,\n",
        "                                             units, mask_zero=True)\n",
        "\n",
        "    # 2. Use LSTM for the decoder\n",
        "    self.rnn = tf.keras.layers.LSTM(units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    # 3. The RNN output will be the query for the attention layer.\n",
        "    self.attention = CrossAttention(units)\n",
        "\n",
        "    # 4. This fully connected layer produces the logits for each\n",
        "    # output token.\n",
        "    self.output_layer = tf.keras.layers.Dense(self.vocab_size)\n",
        "\n",
        "  def call(self,\n",
        "         context, x,\n",
        "         state=None,\n",
        "         return_state=False):  \n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(x, 'batch t')\n",
        "    shape_checker(context, 'batch s units')\n",
        "\n",
        "    # 1. Lookup the embeddings\n",
        "    x = self.embedding(x)\n",
        "    shape_checker(x, 'batch t units')\n",
        "\n",
        "    # 2. Process the target sequence with LSTM\n",
        "    # LSTM returns outputs, h_state, c_state\n",
        "    if state is None:\n",
        "      x, h_state, c_state = self.rnn(x)\n",
        "    else:\n",
        "      # Unpack the state list [h_state, c_state]\n",
        "      x, h_state, c_state = self.rnn(x, initial_state=state)\n",
        "      \n",
        "    shape_checker(x, 'batch t units')\n",
        "\n",
        "    # 3. Use the RNN output as the query for the attention over the context.\n",
        "    x = self.attention(x, context)\n",
        "    self.last_attention_weights = self.attention.last_attention_weights\n",
        "    shape_checker(x, 'batch t units')\n",
        "    shape_checker(self.last_attention_weights, 'batch t s')\n",
        "\n",
        "    # Step 4. Generate logit predictions for the next token.\n",
        "    logits = self.output_layer(x)\n",
        "    shape_checker(logits, 'batch t target_vocab_size')\n",
        "\n",
        "    if return_state:\n",
        "      return logits, [h_state, c_state]\n",
        "    else:\n",
        "      return logits\n",
        "  \n",
        "  def get_initial_state(self, context):\n",
        "    batch_size = tf.shape(context)[0]\n",
        "    start_tokens = tf.fill([batch_size, 1], self.start_token)\n",
        "    done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "    embedded = self.embedding(start_tokens)\n",
        "    \n",
        "    # Create both h_state and c_state for LSTM\n",
        "    h_state = tf.zeros([batch_size, self.units])\n",
        "    c_state = tf.zeros([batch_size, self.units])\n",
        "    \n",
        "    return start_tokens, done, [h_state, c_state]\n",
        "  \n",
        "  def tokens_to_text(self, tokens):\n",
        "    words = self.id_to_word(tokens)\n",
        "    result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
        "    result = tf.strings.regex_replace(result, '^ *\\\\[START\\\\] *', '')\n",
        "    result = tf.strings.regex_replace(result, ' *\\\\[END\\\\] *$', '')\n",
        "    # Add this line to fix punctuation spacing\n",
        "    result = tf.strings.regex_replace(result, ' ([.?!,])', '\\\\1')\n",
        "    return result\n",
        "  \n",
        "  def get_next_token(self, context, next_token, done, state, temperature = 0.0):\n",
        "    logits, state = self(\n",
        "      context, next_token,\n",
        "      state = state,\n",
        "      return_state=True) \n",
        "    \n",
        "    if temperature == 0.0:\n",
        "      next_token = tf.argmax(logits, axis=-1)\n",
        "    else:\n",
        "      logits = logits[:, -1, :]/temperature\n",
        "      next_token = tf.random.categorical(logits, num_samples=1)\n",
        "\n",
        "    # If a sequence produces an `end_token`, set it `done`\n",
        "    done = done | (next_token == self.end_token)\n",
        "    # Once a sequence is done it only produces 0-padding.\n",
        "    next_token = tf.where(done, tf.constant(0, dtype=tf.int64), next_token)\n",
        "    \n",
        "    return next_token, done, state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "4ZUMbYXIEVeA"
      },
      "outputs": [],
      "source": [
        "decoder = Decoder(target_text_processor, CONFIG[\"UNITS\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "5YM-lD7bzx18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "encoder output shape: (batch, s, units) (64, 19, 256)\n",
            "input target tokens shape: (batch, t) (64, 18)\n",
            "logits shape shape: (batch, target_vocabulary_size) (64, 18, 5000)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/anaconda3/envs/cs681-3/lib/python3.11/site-packages/keras/src/layers/layer.py:396: UserWarning: `build()` was called on layer 'decoder_4', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/opt/homebrew/anaconda3/envs/cs681-3/lib/python3.11/site-packages/keras/src/layers/layer.py:940: UserWarning: Layer 'cross_attention_4' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "logits = decoder(ex_context, ex_tar_in)\n",
        "\n",
        "print(f'encoder output shape: (batch, s, units) {ex_context.shape}')\n",
        "print(f'input target tokens shape: (batch, t) {ex_tar_in.shape}')\n",
        "print(f'logits shape shape: (batch, target_vocabulary_size) {logits.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "SuehagxL-JBZ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([b'ikutilah jawabnya rokok runtuh lift hadir setrika hanya terkaya memberitahunya',\n",
              "       b'hilang bercerai dituntaskan esok dipertahankan rinci diduga sumur situsnya tebal',\n",
              "       b'jawabanmu memberitahukanmu pilih piringnya perhatianku sekencangkencangnya depanmu mewawancarai merdeka kopi'],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Setup the loop variables.\n",
        "next_token, done, state = decoder.get_initial_state(ex_context)\n",
        "tokens = []\n",
        "\n",
        "for n in range(10):\n",
        "  # Run one step.\n",
        "  next_token, done, state = decoder.get_next_token(\n",
        "      ex_context, next_token, done, state, temperature=1.0)\n",
        "  # Add the token to the output.\n",
        "  tokens.append(next_token)\n",
        "\n",
        "# Stack all the tokens together.\n",
        "tokens = tf.concat(tokens, axis=-1) # (batch, t)\n",
        "\n",
        "# Convert the tokens back to a a string\n",
        "result = decoder.tokens_to_text(tokens)\n",
        "result[:3].numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "WWIyuy71TkJT"
      },
      "outputs": [],
      "source": [
        "class Translator(tf.keras.Model):\n",
        "  def __init__(self, units,\n",
        "               context_text_processor,\n",
        "               target_text_processor):\n",
        "    super().__init__()\n",
        "    # Build the encoder and decoder\n",
        "    encoder = Encoder(context_text_processor, units)\n",
        "    decoder = Decoder(target_text_processor, units)\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def call(self, inputs):\n",
        "    context, x = inputs\n",
        "    context = self.encoder(context)\n",
        "    logits = self.decoder(context, x)\n",
        "\n",
        "    try:\n",
        "      # Delete the keras mask, so keras doesn't scale the loss+accuracy. \n",
        "      del logits._keras_mask\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "    return logits\n",
        "  \n",
        "  def translate(self,\n",
        "    texts, *,\n",
        "    max_length=CONFIG[\"MAX_LENGTH\"],\n",
        "    temperature=CONFIG[\"TEMPERATURE\"]):\n",
        "    # Process the input texts\n",
        "    context = self.encoder.convert_input(texts)\n",
        "    batch_size = tf.shape(texts)[0]\n",
        "\n",
        "    # Setup the loop inputs\n",
        "    tokens = []\n",
        "    attention_weights = []\n",
        "    next_token, done, state = self.decoder.get_initial_state(context)\n",
        "\n",
        "    for _ in range(max_length):\n",
        "      # Generate the next token\n",
        "      next_token, done, state = self.decoder.get_next_token(\n",
        "          context, next_token, done,  state, temperature)\n",
        "          \n",
        "      # Collect the generated tokens\n",
        "      tokens.append(next_token)\n",
        "      attention_weights.append(self.decoder.last_attention_weights)\n",
        "      \n",
        "      if tf.executing_eagerly() and tf.reduce_all(done):\n",
        "        break\n",
        "\n",
        "    # Stack the lists of tokens and attention weights.\n",
        "    tokens = tf.concat(tokens, axis=-1)   # t*[(batch 1)] -> (batch, t)\n",
        "    self.last_attention_weights = tf.concat(attention_weights, axis=1)  # t*[(batch 1 s)] -> (batch, t s)\n",
        "\n",
        "    result = self.decoder.tokens_to_text(tokens)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "8vhjTh84K6Mg"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/anaconda3/envs/cs681-3/lib/python3.11/site-packages/keras/src/layers/layer.py:396: UserWarning: `build()` was called on layer 'decoder_5', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/opt/homebrew/anaconda3/envs/cs681-3/lib/python3.11/site-packages/keras/src/layers/layer.py:940: UserWarning: Layer 'cross_attention_5' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context tokens, shape: (batch, s, units) (64, 19)\n",
            "Target tokens, shape: (batch, t) (64, 18)\n",
            "logits, shape: (batch, t, target_vocabulary_size) (64, 18, 5000)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/anaconda3/envs/cs681-3/lib/python3.11/site-packages/keras/src/layers/layer.py:940: UserWarning: Layer 'decoder_5' (of type Decoder) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model = Translator(\n",
        "    CONFIG[\"UNITS\"], \n",
        "    context_text_processor, \n",
        "    target_text_processor)\n",
        "\n",
        "logits = model((ex_context_tok, ex_tar_in))\n",
        "\n",
        "print(f'Context tokens, shape: (batch, s, units) {ex_context_tok.shape}')\n",
        "print(f'Target tokens, shape: (batch, t) {ex_tar_in.shape}')\n",
        "print(f'logits, shape: (batch, t, target_vocabulary_size) {logits.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"translator_2\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"translator_2\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ encoder_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)             │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,208,512</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)             │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,142,728</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ encoder_5 (\u001b[38;5;33mEncoder\u001b[0m)             │ ?                      │     \u001b[38;5;34m2,208,512\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ decoder_5 (\u001b[38;5;33mDecoder\u001b[0m)             │ ?                      │     \u001b[38;5;34m4,142,728\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,351,240</span> (24.23 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,351,240\u001b[0m (24.23 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,351,240</span> (24.23 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,351,240\u001b[0m (24.23 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "WmTHr5iV3jFr"
      },
      "outputs": [],
      "source": [
        "def masked_loss(y_true, y_pred):\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "    loss = loss_fn(y_true, y_pred)\n",
        "\n",
        "    # Mask off the losses on padding.\n",
        "    mask = tf.cast(y_true != 0, loss.dtype)\n",
        "    loss *= mask\n",
        "\n",
        "    # Return the total.\n",
        "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "nRB1CTmQWOIL"
      },
      "outputs": [],
      "source": [
        "def masked_acc(y_true, y_pred):\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    y_pred = tf.argmax(y_pred, axis=-1)\n",
        "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
        "    \n",
        "    match = tf.cast(y_true == y_pred, tf.float32)\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "    \n",
        "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "9g0DRRvm3l9X"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=CONFIG[\"OPTIMIZER\"],\n",
        "    loss=masked_loss, \n",
        "    metrics=[masked_acc, masked_loss])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "8rJITfxEsHKR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - loss: 8.5129 - masked_acc: 1.4781e-04 - masked_loss: 8.5129\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'loss': 8.512399673461914,\n",
              " 'masked_acc': 9.920635784510523e-05,\n",
              " 'masked_loss': 8.512399673461914}"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(val_ds, steps=20, return_dict=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "BQd_esVVoSf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m 32/100\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 208ms/step - loss: 6.8715 - masked_acc: 0.1305 - masked_loss: 6.8715"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[95]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEPOCHS\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSTEPS_PER_EPOCH\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mVALIDATION_STEPS\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEARLY_STOPPING_PATIENCE\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/cs681-3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/cs681-3/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    370\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/cs681-3/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    217\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    218\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    221\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/cs681-3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/cs681-3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/cs681-3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/cs681-3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/cs681-3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/cs681-3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/cs681-3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/cs681-3/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/cs681-3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    train_ds.repeat(), \n",
        "    epochs=CONFIG[\"EPOCHS\"],\n",
        "    steps_per_epoch=CONFIG[\"STEPS_PER_EPOCH\"],\n",
        "    validation_data=val_ds,\n",
        "    validation_steps=CONFIG[\"VALIDATION_STEPS\"],\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            patience=CONFIG[\"EARLY_STOPPING_PATIENCE\"])])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-FLCjBVEMXL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing 5 random samples from the test set:\n",
            "==================================================\n",
            "Input: I can't forgive her.\n",
            "Translation: aku tidak bisa memaafkannya. \n",
            "==================================================\n",
            "Input: I don't want to live with you.\n",
            "Translation: aku tidak ingin tinggal dengan kamu. \n",
            "==================================================\n",
            "Input: You can fix it, can't you?\n",
            "Translation: kamu bisa melakukannya, kan? \n",
            "==================================================\n",
            "Input: Tom just wanted to help.\n",
            "Translation: tom hanya ingin membantu. \n",
            "==================================================\n",
            "Input: Do you like carrots?\n",
            "Translation: apakah kamu suka cokelat? \n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Define constants\n",
        "SEPARATOR_LINE = \"=\" * 50\n",
        "\n",
        "# Create test pairs from the validation data\n",
        "is_test = np.random.uniform(size=(len(target_raw),)) >= 0.8\n",
        "test_context = context_raw[is_test]\n",
        "test_target = target_raw[is_test]\n",
        "test_pairs = list(zip(test_context, test_target))\n",
        "\n",
        "# Get the English texts\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "\n",
        "# Display both input and expected translation\n",
        "print(f\"Testing {CONFIG['TEST_SAMPLE_COUNT']} random samples from the test set:\")\n",
        "for _ in range(CONFIG[\"TEST_SAMPLE_COUNT\"]):\n",
        "    # Select a random English sentence\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    \n",
        "    # Get the corresponding Indonesian translation from our model\n",
        "    translation = model.translate([input_sentence])[0].numpy().decode()\n",
        "    \n",
        "    # Print results\n",
        "    print(SEPARATOR_LINE)\n",
        "    print(f\"Input: {input_sentence}\")\n",
        "    print(f\"Translation: {translation}\")\n",
        "\n",
        "print(SEPARATOR_LINE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating model on test set...\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - loss: 1.9037 - masked_acc: 0.6605 - masked_loss: 1.9059\n",
            "\n",
            "Test accuracy: 0.5915 (59.15%)\n",
            "Test loss: 2.2961\n",
            "\n",
            "Comparison with validation metrics:\n",
            "Validation accuracy: 0.5751 (57.51%)\n",
            "Test accuracy:       0.5915 (59.15%)\n"
          ]
        }
      ],
      "source": [
        "# Create test dataset from the same split used earlier\n",
        "test_raw = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((context_raw[~is_train], target_raw[~is_train]))\n",
        "    .batch(CONFIG[\"BATCH_SIZE\"])\n",
        ")\n",
        "\n",
        "test_ds = test_raw.map(process_text, tf.data.AUTOTUNE)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "print(\"Evaluating model on test set...\")\n",
        "test_results = model.evaluate(test_ds, return_dict=True)\n",
        "\n",
        "print(f\"\\nTest accuracy: {test_results['masked_acc']:.4f} ({test_results['masked_acc']*100:.2f}%)\")\n",
        "print(f\"Test loss: {test_results['loss']:.4f}\")\n",
        "\n",
        "# Compare with validation metrics\n",
        "print(\"\\nComparison with validation metrics:\")\n",
        "val_results = model.evaluate(val_ds, steps=20, return_dict=True, verbose=0)\n",
        "print(f\"Validation accuracy: {val_results['masked_acc']:.4f} ({val_results['masked_acc']*100:.2f}%)\")\n",
        "print(f\"Test accuracy:       {test_results['masked_acc']:.4f} ({test_results['masked_acc']*100:.2f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on 100 samples (out of 2681 total)\n",
            "\n",
            "Showing example translations:\n",
            "Input: the brakes don't work.\n",
            "Candidate: [UNK] [UNK]. \n",
            "Reference: remnya tidak berfungsi.\n",
            "BLEU Score: 0.0000 using BLEU-1 (short sentence)\n",
            "\n",
            "Input: he's a man you can rely on.\n",
            "Candidate: dia orang bisa bisa anda. \n",
            "Reference: dia pria yang bisa diandalkan.\n",
            "BLEU Score: 0.0639 using BLEU-4\n",
            "\n",
            "Input: it's confidential.\n",
            "Candidate: ini rahasia. \n",
            "Reference: ini rahasia.\n",
            "BLEU Score: 1.0000 using BLEU-1 (short sentence)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/anaconda3/envs/cs681-3/lib/python3.11/site-packages/keras/src/layers/layer.py:940: UserWarning: Layer 'cross_attention_1' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/opt/homebrew/anaconda3/envs/cs681-3/lib/python3.11/site-packages/keras/src/layers/layer.py:940: UserWarning: Layer 'decoder_1' (of type Decoder) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: tom was worn out.\n",
            "Candidate: tom ingin. \n",
            "Reference: tom kelelahan.\n",
            "BLEU Score: 0.5000 using BLEU-1 (short sentence)\n",
            "\n",
            "Input: have you ever been to mexico?\n",
            "Candidate: kamu pernah ke meksiko? \n",
            "Reference: apakah kamu pernah ke meksiko?\n",
            "BLEU Score: 0.7788 using BLEU-4\n",
            "\n",
            "Calculating score...\n",
            "Progress: 100.0% (100/100)\n",
            "Calculation complete!                \n",
            "Average BLEU score: 0.2737\n"
          ]
        }
      ],
      "source": [
        "# BLEU evaluation cell\n",
        "import random\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "def evaluate_bleu(model, context_raw, target_raw, \n",
        "                 sample_size=CONFIG[\"BLEU_SAMPLE_SIZE\"], \n",
        "                 verbose=True):\n",
        "    \"\"\"\n",
        "    Evaluate BLEU score on a sample of test pairs\n",
        "    \n",
        "    Args:\n",
        "        model: Trained Translator model\n",
        "        context_raw: Array of English sentences\n",
        "        target_raw: Array of Indonesian sentences\n",
        "        sample_size: Number of examples to evaluate (None = all)\n",
        "        verbose: Whether to print detailed results\n",
        "    \n",
        "    Returns:\n",
        "        Average BLEU score\n",
        "    \"\"\"\n",
        "    # Create test pairs from the validation data\n",
        "    is_test = np.random.uniform(size=(len(target_raw),)) >= 0.8\n",
        "    test_context = context_raw[is_test]\n",
        "    test_target = target_raw[is_test]\n",
        "    test_pairs = list(zip(test_context, test_target))\n",
        "    \n",
        "    # Create a sample if specified\n",
        "    if sample_size and sample_size < len(test_pairs):\n",
        "        sampled_pairs = random.sample(test_pairs, sample_size)\n",
        "        if verbose:\n",
        "            print(f\"Evaluating on {sample_size} samples (out of {len(test_pairs)} total)\")\n",
        "    else:\n",
        "        sampled_pairs = test_pairs\n",
        "        if verbose:\n",
        "            print(f\"Evaluating on full test set ({len(test_pairs)} examples)\")\n",
        "    \n",
        "    test_eng_texts = [pair[0].lower() for pair in sampled_pairs]\n",
        "    test_ind_texts = [pair[1].lower() for pair in sampled_pairs]\n",
        "    smoother = SmoothingFunction().method1\n",
        "    \n",
        "    # Display examples if verbose\n",
        "    if verbose:\n",
        "        print(\"\\nShowing example translations:\")\n",
        "        for i in range(min(len(sampled_pairs), 5)):  # Show up to 5 examples\n",
        "            # Use model.translate method for translation\n",
        "            candidate = model.translate([test_eng_texts[i]])[0].numpy().decode()\n",
        "            reference = test_ind_texts[i]\n",
        "            \n",
        "            # Clean up tokens for fair comparison\n",
        "            reference_tokens = reference.strip().split()\n",
        "            candidate_tokens = candidate.strip().split()\n",
        "            \n",
        "            print(f\"Input: {test_eng_texts[i]}\")\n",
        "            print(f\"Candidate: {candidate}\")\n",
        "            print(f\"Reference: {reference}\")\n",
        "            \n",
        "            if len(candidate_tokens) == 0:\n",
        "                score = 0.0\n",
        "                print(f\"BLEU Score: {score:.4f} (empty translation)\")\n",
        "            else:\n",
        "                # Choose weights based on reference sentence length\n",
        "                if len(reference_tokens) < 4:\n",
        "                    # Use BLEU-1 for short sentences (unigrams only)\n",
        "                    weights = (1.0, 0.0, 0.0, 0.0)\n",
        "                    metric_type = \"BLEU-1 (short sentence)\"\n",
        "                else:\n",
        "                    # Use standard BLEU-4 for longer sentences\n",
        "                    weights = (0.25, 0.25, 0.25, 0.25)\n",
        "                    metric_type = \"BLEU-4\"\n",
        "                    \n",
        "                score = sentence_bleu([reference_tokens], candidate_tokens,\n",
        "                                    weights=weights,\n",
        "                                    smoothing_function=smoother)\n",
        "                print(f\"BLEU Score: {score:.4f} using {metric_type}\\n\")\n",
        "    \n",
        "    # Calculate BLEU on entire sample\n",
        "    print(\"Calculating score...\")\n",
        "    total_bleu = 0\n",
        "    count = len(sampled_pairs)\n",
        "    \n",
        "    for i in range(count):\n",
        "        progress = (i + 1) / count * 100\n",
        "        if verbose:\n",
        "            print(f\"Progress: {progress:.1f}% ({i+1}/{count})\", end=\"\\r\")\n",
        "            \n",
        "        # Use model.translate method for translation\n",
        "        candidate = model.translate([test_eng_texts[i]])[0].numpy().decode()\n",
        "        reference = test_ind_texts[i]\n",
        "        \n",
        "        # Clean up tokens for fair comparison\n",
        "        reference_tokens = reference.strip().split()\n",
        "        candidate_tokens = candidate.strip().split()\n",
        "        \n",
        "        if len(candidate_tokens) == 0:\n",
        "            score = 0.0\n",
        "        else:\n",
        "            # Choose weights based on sentence length\n",
        "            if len(reference_tokens) < 4:\n",
        "                # Use BLEU-1 for short sentences (unigrams only)\n",
        "                weights = (1.0, 0.0, 0.0, 0.0)\n",
        "            else:\n",
        "                # Use standard BLEU-4 for longer sentences\n",
        "                weights = (0.25, 0.25, 0.25, 0.25)\n",
        "                \n",
        "            score = sentence_bleu([reference_tokens], candidate_tokens,\n",
        "                                 weights=weights,\n",
        "                                 smoothing_function=smoother)\n",
        "        \n",
        "        total_bleu += score\n",
        "    \n",
        "    avg_bleu = total_bleu / count\n",
        "    print(\"\\nCalculation complete!                \")\n",
        "    print(f\"Average BLEU score: {avg_bleu:.4f}\")\n",
        "    return avg_bleu\n",
        "\n",
        "# Evaluate with a sample of test pairs\n",
        "quick_bleu = evaluate_bleu(model, context_raw, target_raw, sample_size=CONFIG[\"BLEU_SAMPLE_SIZE\"])\n",
        "\n",
        "# For final evaluation with more samples\n",
        "# full_bleu = evaluate_bleu(model, context_raw, target_raw, sample_size=500)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "name": "nmt_with_attention.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cs681-3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
